{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDiBmrK21NEO"
   },
   "source": [
    "##### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xisiN_Ll03Kj",
    "outputId": "11cdb4c9-ca3d-4fe4-bffb-010f4ea1904f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "!pip install h-transformer-1d --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HlfOivg1OqX"
   },
   "source": [
    "---\n",
    "##### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDjNyLb11Bi8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if(\"data\" not in os.listdir()):\n",
    "  !mkdir ./data\n",
    "# Officially the gzipped version of the data is not available anymore, only zip\n",
    "# The \"with gzip.open('file.gz') as file:\" has to be adapted then\n",
    "#  !wget -c https://data.deepai.org/enwik8.zip -P ./data/\n",
    "!wget -c https://github.com/lucidrains/h-transformer-1d/raw/main/data/enwik8.gz -P ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpcJG1Jc1Qpu"
   },
   "source": [
    "---\n",
    "##### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "G7HuiXln1De4",
    "outputId": "1c9cd47d-943f-4582-cf88-71deef4671ca"
   },
   "outputs": [],
   "source": [
    "from h_transformer_1d import HTransformer1D\n",
    "from h_transformer_1d.autoregressive_wrapper import AutoregressiveWrapper\n",
    "\n",
    "import random\n",
    "import tqdm\n",
    "import gzip\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# constants\n",
    "\n",
    "NUM_BATCHES = int(1e5)\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATE_EVERY = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "VALIDATE_EVERY  = 100\n",
    "GENERATE_EVERY  = 500\n",
    "GENERATE_LENGTH = 512\n",
    "SEQ_LEN = 4096\n",
    "\n",
    "# helpers\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def decode_token(token):\n",
    "    return str(chr(max(32, token)))\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    return ''.join(list(map(decode_token, tokens)))\n",
    "\n",
    "# instantiate GPT-like decoder model\n",
    "\n",
    "model = HTransformer1D(\n",
    "    num_tokens = 256,\n",
    "    dim = 512,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    depth = 8,\n",
    "    heads = 8,\n",
    "    causal = True,\n",
    "    reversible = True\n",
    ")\n",
    "\n",
    "model = AutoregressiveWrapper(model)\n",
    "model.cuda()\n",
    "\n",
    "# prepare enwik8 data\n",
    "\n",
    "with gzip.open('./data/enwik8.gz') as file:\n",
    "    X = np.fromstring(file.read(int(95e6)), dtype=np.uint8)\n",
    "    trX, vaX = np.split(X, [int(90e6)])\n",
    "    data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)\n",
    "\n",
    "class TextSamplerDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rand_start = torch.randint(0, self.data.size(0) - self.seq_len, (1,))\n",
    "        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n",
    "        return full_seq.cuda()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) // self.seq_len\n",
    "\n",
    "train_dataset = TextSamplerDataset(data_train, SEQ_LEN)\n",
    "val_dataset   = TextSamplerDataset(data_val, SEQ_LEN)\n",
    "train_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\n",
    "val_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# training\n",
    "\n",
    "for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10., desc='training'):\n",
    "    model.train()\n",
    "\n",
    "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        loss = model(next(train_loader))\n",
    "        loss.backward()\n",
    "\n",
    "    print(\"training loss: {:.4f}\".format(loss.item()))\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss = model(next(val_loader))\n",
    "            print(f'validation loss: {loss.item()}')\n",
    "\n",
    "    if i % GENERATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        inp = random.choice(val_dataset)[:-1]\n",
    "        prime = decode_tokens(inp)\n",
    "        print('{} \\n {} \\n'.format(prime, '*' * 100))\n",
    "\n",
    "        sample = model.generate(inp, GENERATE_LENGTH)\n",
    "        output_str = decode_tokens(sample)\n",
    "        print(output_str)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "H-Transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "pycharm-9e81eaed",
   "language": "python",
   "display_name": "PyCharm (Z39gYQDck4L3hkYZ)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}